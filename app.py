# app.py
import os
import io
import re
import time
from pathlib import Path
from typing import Any, Dict, Optional, List

import pandas as pd
import streamlit as st
import yaml
from openai import OpenAI

import processing as proc  # processing.py —Ä—è–¥–æ–º —Å app.py

# –í–ê–ñ–ù–û: —ç—Ç–∏ –¥–≤–∞ —Ñ–∞–π–ª–∞ –¥–æ–ª–∂–Ω—ã –ª–µ–∂–∞—Ç—å —Ä—è–¥–æ–º —Å app.py:
# - llm_model.py  (–∫–ª–∞—Å—Å OpenAIRelevanceBatchModel)
# - filter_service.py (–∫–ª–∞—Å—Å RelevanceFilterService)
try:
    from llm_model import OpenAIRelevanceBatchModel
    from filter_service import RelevanceFilterService
except Exception as e:
    OpenAIRelevanceBatchModel = None
    RelevanceFilterService = None
    _IMPORT_ERR = e
else:
    _IMPORT_ERR = None

# Sentiment stack (–º–æ–∂–µ—Ç –æ—Ç—Å—É—Ç—Å—Ç–≤–æ–≤–∞—Ç—å/–ª–æ–º–∞—Ç—å—Å—è –∏–∑-–∑–∞ deps ‚Äî —Ç–æ–≥–¥–∞ –ø—Ä–æ—Å—Ç–æ –≤—ã–∫–ª—é—á–∏–º —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª)
try:
    from embedders import OpenAIEmbedder
    from sentiment_model import SentimentModel, SentimentModelConfig
    from sentiment_service import SentimentService
except Exception as e:
    OpenAIEmbedder = None
    SentimentModel = None
    SentimentModelConfig = None
    SentimentService = None
    _SENTIMENT_IMPORT_ERR = e
else:
    _SENTIMENT_IMPORT_ERR = None

try:
    from config_local import OPENAI_API_KEY as LOCAL_OPENAI_API_KEY
except Exception:
    LOCAL_OPENAI_API_KEY = None


# ---------------- UI CONFIG ----------------
st.set_page_config(
    page_title="Brand Analytics (MVP)",
    page_icon="üßº",
    layout="centered",
)

st.markdown(
    """
<style>
.block-container { padding-top: 2rem; max-width: 980px; }
.small-note { opacity: 0.75; font-size: 0.92rem; }

.card {
  background: white;
  border: 1px solid rgba(0,0,0,0.06);
  border-radius: 16px;
  padding: 16px 16px 10px 16px;
  box-shadow: 0 6px 20px rgba(0,0,0,0.04);
  margin-bottom: 14px;
}

/* ====== BADGES (unified) ====== */
.badge {
  display: inline-flex;
  align-items: center;
  justify-content: center;

  padding: 6px 12px;
  border-radius: 999px;

  font-weight: 800;
  font-size: 0.9rem;
  line-height: 1;

  border: 1px solid rgba(0,0,0,0.10);
  color: #111827;
  background: rgba(17,24,39,0.06);

  margin-right: 10px; /* –æ–¥–∏–Ω–∞–∫–æ–≤—ã–π –æ—Ç—Å—Ç—É–ø –º–µ–∂–¥—É –±–µ–π–¥–∂–∞–º–∏ */
}

/* Green (KEEP / POSITIVE) */
.badge--green { background: rgba(34,197,94,0.12); }

/* Red (DROP / NEGATIVE) */
.badge--red { background: rgba(239,68,68,0.12); }

/* Neutral (NEUTRAL) */
.badge--gray { background: rgba(107,114,128,0.14); }

/* Rule (optional) */
.badge--blue { background: rgba(59,130,246,0.12); }

.badge-row {
  display: flex;
  flex-wrap: wrap;
  gap: 10px;
  align-items: center;
  margin-top: 8px;
  margin-bottom: 8px;
}
.badge { margin-right: 0; }
</style>
""",
    unsafe_allow_html=True,
)

DEFAULT_MODEL = "gpt-4.1-mini"

BASE_SYSTEM_TEMPLATE = """
–¢—ã ‚Äî —Ñ–∏–ª—å—Ç—Ä —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –¥–ª—è –±—Ä–µ–Ω–¥–∞ "{brand_name}" –∏ –µ–≥–æ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã—Ö —Ü–∏—Ñ—Ä–æ–≤—ã—Ö –∫–∞–Ω–∞–ª–æ–≤ (–µ—Å–ª–∏ –µ—Å—Ç—å: –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ/—Å–∞–π—Ç/–±–æ–Ω—É—Å—ã).

–ö–æ–Ω—Ç–µ–∫—Å—Ç –±—Ä–µ–Ω–¥–∞:
{brand_description}

–°–∏–Ω–æ–Ω–∏–º—ã/–∞–ª–∏–∞—Å—ã (–∫–∞–∫ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏ –º–æ–≥—É—Ç –ø–∏—Å–∞—Ç—å –±—Ä–µ–Ω–¥):
{brand_aliases}

–ó–∞–¥–∞—á–∞: –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è –≤—ã–±—Ä–∞—Ç—å —Ç–æ–ª—å–∫–æ –æ–¥–Ω–æ:
- "keep" ‚Äî –µ—Å–ª–∏ —Å–æ–æ–±—â–µ–Ω–∏–µ –æ—Ç–Ω–æ—Å–∏—Ç—Å—è –∫ –±—Ä–µ–Ω–¥—É/–º–∞–≥–∞–∑–∏–Ω—É/—Å–µ—Ç–∏/–ø—Ä–∏–ª–æ–∂–µ–Ω–∏—é/–±–æ–Ω—É—Å–∞–º/–ø–æ–∫—É–ø–∫–∞–º/–∞—Å—Å–æ—Ä—Ç–∏–º–µ–Ω—Ç—É/—Å–∫–∏–¥–∫–∞–º/—Å–µ—Ä–≤–∏—Å—É.
- "drop" ‚Äî –µ—Å–ª–∏ —ç—Ç–æ —è–≤–Ω–æ –ù–ï –ø—Ä–æ –±—Ä–µ–Ω–¥ –∫–∞–∫ –º–∞–≥–∞–∑–∏–Ω/–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ/—Å–µ—Ä–≤–∏—Å.

–ü–†–ê–í–ò–õ–û –ü–û –£–ú–û–õ–ß–ê–ù–ò–Æ: –µ—Å–ª–∏ –µ—Å—Ç—å —Å–æ–º–Ω–µ–Ω–∏—è ‚Äî –≤—ã–±–∏—Ä–∞–π "keep".

–í–∞–∂–Ω–æ:
- –í–æ–∑–≤—Ä–∞—â–∞–π —Å—Ç—Ä–æ–≥–æ JSON –ø–æ —Å—Ö–µ–º–µ.
- –ù–∏–∫–∞–∫–∏—Ö –ø–æ—è—Å–Ω–µ–Ω–∏–π, –ø—Ä–∏—á–∏–Ω, —Ç–µ–∫—Å—Ç–∞ ‚Äî —Ç–æ–ª—å–∫–æ JSON.
""".strip()


# ---------------- Helpers: sentiment meta/cache ----------------
def _sentiment_available() -> bool:
    return (OpenAIEmbedder is not None) and (SentimentModel is not None) and (SentimentService is not None)


def _read_sentiment_meta(npz_path: str) -> dict:
    yml = Path(npz_path).with_suffix(".yaml")
    if yml.exists():
        try:
            with open(yml, "r", encoding="utf-8") as f:
                return yaml.safe_load(f) or {}
        except Exception:
            return {}
    return {}


@st.cache_resource
def get_sentiment_model_cached(api_key: str, artifacts_npz: str, openai_embed_model: str, dimensions: int | None):
    """
    –ó–∞–≥—Ä—É–∂–∞–µ–º –ø—Ä–æ—Ç–æ—Ç–∏–ø—ã/–ø–æ—Ä–æ–≥–∏ –∏–∑ .npz –∏ —Å–æ–∑–¥–∞—ë–º –º–æ–¥–µ–ª—å —Å OpenAI embeddings (–¥–ª—è –ø—Ä–æ–¥-–∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞).
    """
    if not _sentiment_available():
        raise RuntimeError(
            f"Sentiment stack is not available: {_SENTIMENT_IMPORT_ERR}")

    client = OpenAI(api_key=api_key)
    embedder = OpenAIEmbedder(
        client=client, model=openai_embed_model, dimensions=dimensions)

    cfg = SentimentModelConfig(enable_llm_fallback=False)
    m = SentimentModel(embed_fn=embedder.embed_texts,
                       config=cfg, openai_api_key=api_key)
    m.load_artifacts(artifacts_npz)
    return m


# ---------------- Helpers: brands ----------------
def load_brands(path: str = "brands.yaml") -> Dict[str, Dict[str, Any]]:
    if not os.path.exists(path):
        return {}
    with open(path, "r", encoding="utf-8") as f:
        data = yaml.safe_load(f) or {}

    out: Dict[str, Dict[str, Any]] = {}
    for k, v in data.items():
        if not isinstance(v, dict):
            continue
        p = dict(v)
        p.setdefault("brand_name", k)
        p.setdefault("description", "")
        p.setdefault("aliases", [])
        # —Å—Ç–∞—Ä—ã–µ –∫–ª—é—á–∏
        p.setdefault("sure_drop_patterns", [])
        p.setdefault("pr_reply_markers", [])
        # –Ω–æ–≤—ã–µ –∫–ª—é—á–∏ (–¥–ª—è —Å–µ—Ä–≤–∏—Å-–∫–ª–∞—Å—Å–∞)
        p.setdefault("brand_sure_drop", p.get("sure_drop_patterns", []))
        p.setdefault("homonym_noise", [])
        out[k] = p
    return out


def normalize_profile(profile: Dict[str, Any]) -> Dict[str, Any]:
    p = dict(profile or {})
    p.setdefault("brand_name", "BRAND")
    p.setdefault("description", "")
    p.setdefault("aliases", [])
    p.setdefault("sure_drop_patterns", [])
    p.setdefault("pr_reply_markers", [])

    # —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å RuleEngine –≤ filter_service.py
    if "brand_sure_drop" not in p or p["brand_sure_drop"] is None:
        p["brand_sure_drop"] = p.get("sure_drop_patterns", [])
    if "homonym_noise" not in p or p["homonym_noise"] is None:
        p["homonym_noise"] = []
    return p


def format_system_prompt(base_template: str, profile: Dict[str, Any]) -> str:
    brand_name = (profile.get("brand_name") or "BRAND").strip()
    desc = (profile.get("description") or "").strip()
    aliases = profile.get("aliases") or []
    aliases_str = ", ".join([a.strip()
                            for a in aliases if str(a).strip()]) or "‚Äî"
    return base_template.format(
        brand_name=brand_name,
        brand_description=desc if desc else "‚Äî",
        brand_aliases=aliases_str,
    ).strip()


# ---------------- Helpers: OpenAI ----------------
def get_api_key() -> Optional[str]:
    secret_key = None
    try:
        secret_key = st.secrets.get("OPENAI_API_KEY", None)
    except Exception:
        secret_key = None
    return secret_key or os.getenv("OPENAI_API_KEY") or LOCAL_OPENAI_API_KEY


@st.cache_resource
def get_client() -> OpenAI:
    api_key = get_api_key()
    return OpenAI(api_key=api_key or "")


# ---------------- Helpers: file IO ----------------
def read_uploaded_table(uploaded_file) -> pd.DataFrame:
    name = (uploaded_file.name or "").lower()
    if name.endswith((".xlsx", ".xls")):
        return pd.read_excel(uploaded_file, engine="openpyxl")

    data = uploaded_file.getvalue()
    bio = io.BytesIO(data)
    try:
        return pd.read_csv(bio, sep=None, engine="python", encoding="utf-8-sig")
    except Exception:
        bio = io.BytesIO(data)
        try:
            return pd.read_csv(bio, sep=";", encoding="utf-8-sig")
        except Exception:
            bio = io.BytesIO(data)
            return pd.read_csv(bio, sep=",", encoding="utf-8-sig")


def df_to_download_bytes(df: pd.DataFrame, out_fmt: str) -> tuple[bytes, str]:
    if out_fmt == "xlsx":
        buff = io.BytesIO()
        with pd.ExcelWriter(buff, engine="openpyxl") as writer:
            df.to_excel(writer, index=False, sheet_name="result")
        return buff.getvalue(), "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
    return df.to_csv(index=False).encode("utf-8-sig"), "text/csv"


# ---------------- Preprocessing factory (uses processing.py) ----------------
def _literal_to_pattern(s: str) -> str:
    s = (s or "").strip()
    if not s:
        return ""
    esc = re.escape(s)
    return rf"(?<!\w){esc}(?!\w)"


def build_brand_patterns(brand_name: str, aliases: List[str], extra_regex: str) -> List[str]:
    pats: List[str] = []
    for t in [brand_name] + (aliases or []):
        p = _literal_to_pattern(t)
        if p:
            pats.append(p)

    for line in (extra_regex or "").splitlines():
        line = line.strip()
        if line:
            pats.append(line)

    if not pats:
        pats = [r"(brand|brands)"]
    return pats


class PreprocessorFactory:
    def __init__(self, max_words: int = 250):
        self.max_words = max_words

    def make(self, profile: Dict[str, Any], extra_brand_patterns: str) -> proc.CommentPreprocessor:
        brand_name = profile.get("brand_name") or "BRAND"
        aliases = profile.get("aliases") or []
        pats = build_brand_patterns(brand_name, aliases, extra_brand_patterns)

        # –º–µ–Ω—è–µ–º –≥–ª–æ–±–∞–ª—å–Ω—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é processing.py "–Ω–∞ –ª–µ—Ç—É"
        proc.BRAND_PATTERNS = pats

        return proc.CommentPreprocessor(
            BRAND_PATTERNS=proc.BRAND_PATTERNS,
            NOISE_PHRASES=proc.NOISE_PHRASES,
            RU_STOP=proc.RU_STOP,
            TOPIC_KEYWORDS=proc.TOPIC_KEYWORDS,
            max_len=self.max_words,  # —É –∫–ª–∞—Å—Å–∞ max_len —Ç—Ä–∞–∫—Ç—É–µ—Ç—Å—è –∫–∞–∫ —á–∏—Å–ª–æ —Å–ª–æ–≤
        )

    def preprocess_for_llm(self, text_rule: str, pre: proc.CommentPreprocessor) -> str:
        out = pre.preprocess(text_rule, max_len=self.max_words)
        return out if out else text_rule


# ---------------- App (class-based) ----------------
class StreamlitRelevanceApp:
    def __init__(self):
        self.brands = load_brands("brands.yaml")
        self.api_key_present = bool(get_api_key())

    def sidebar_settings(self) -> Dict[str, Any]:
        with st.sidebar:
            st.subheader("–ù–∞—Å—Ç—Ä–æ–π–∫–∏")
            model = st.text_input(
                "Model", value=st.session_state.get("model", DEFAULT_MODEL))
            temperature = st.slider(
                "Temperature", 0.0, 1.0, float(
                    st.session_state.get("temperature", 0.0)), 0.1
            )
            truncate_chars = st.number_input(
                "Truncate chars",
                min_value=100,
                max_value=5000,
                value=int(st.session_state.get("truncate_chars", 800)),
                step=50,
            )

            st.session_state["model"] = model
            st.session_state["temperature"] = temperature
            st.session_state["truncate_chars"] = truncate_chars

            brand_names = ["(manual)"] + sorted(list(self.brands.keys()))
            chosen = st.selectbox("–ö–æ–º–ø–∞–Ω–∏—è", brand_names, index=int(
                st.session_state.get("chosen_idx", 0)))
            st.session_state["chosen_idx"] = brand_names.index(chosen)

            st.markdown(
                '<div class="small-note">–ü–æ–¥ ¬´manual¬ª –º–æ–∂–Ω–æ –≤—Å—Ç–∞–≤–∏—Ç—å –∫–∞—Ä—Ç–æ—á–∫—É –±—Ä–µ–Ω–¥–∞ —Ä—É–∫–∞–º–∏.</div>',
                unsafe_allow_html=True,
            )

            st.subheader("–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ (–¥–ª—è —Ñ–∞–π–ª–æ–≤)")
            batch_size = st.number_input(
                "Batch size", 1, 50, int(
                    st.session_state.get("batch_size", 6)), 1
            )
            max_workers = st.number_input(
                "Max workers", 1, 20, int(
                    st.session_state.get("max_workers", 3)), 1
            )
            st.session_state["batch_size"] = int(batch_size)
            st.session_state["max_workers"] = int(max_workers)

            st.subheader("Sentiment")
            enable_sentiment = st.checkbox(
                "Run sentiment after relevance",
                value=bool(st.session_state.get("enable_sentiment", True)),
            )
            sentiment_only_kept = st.checkbox(
                "Analyze only kept (is_drop=No)",
                value=bool(st.session_state.get("sentiment_only_kept", True)),
            )
            sentiment_artifacts = st.text_input(
                "Sentiment artifacts (.npz)",
                value=st.session_state.get(
                    "sentiment_artifacts", "sentiment_assets/sentiment_openai.npz"),
            )

            meta = _read_sentiment_meta(sentiment_artifacts)
            default_embed_model = (
                (meta.get("embedding", {}) or {}).get(
                    "model") or "text-embedding-3-small"
            )
            sentiment_embed_model = st.text_input(
                "OpenAI embedding model",
                value=st.session_state.get(
                    "sentiment_embed_model", default_embed_model),
            )
            sentiment_embed_batch = st.number_input(
                "Embedding batch size", 16, 512, int(
                    st.session_state.get("sentiment_embed_batch", 128)), 16
            )

            st.session_state["enable_sentiment"] = bool(enable_sentiment)
            st.session_state["sentiment_only_kept"] = bool(sentiment_only_kept)
            st.session_state["sentiment_artifacts"] = str(sentiment_artifacts)
            st.session_state["sentiment_embed_model"] = str(
                sentiment_embed_model)
            st.session_state["sentiment_embed_batch"] = int(
                sentiment_embed_batch)

        return {
            "model": model,
            "temperature": float(temperature),
            "truncate_chars": int(truncate_chars),
            "chosen": chosen,
            "batch_size": int(batch_size),
            "max_workers": int(max_workers),
            "enable_sentiment": bool(enable_sentiment),
            "sentiment_only_kept": bool(sentiment_only_kept),
            "sentiment_artifacts": str(sentiment_artifacts),
            "sentiment_embed_model": str(sentiment_embed_model),
            "sentiment_embed_batch": int(sentiment_embed_batch),
        }

    def brand_profile_editor(self, chosen: str) -> tuple[Dict[str, Any], str]:
        # –±–µ—Ä—ë–º –ø—Ä–æ—Ñ–∏–ª—å –∏–∑ —Ñ–∞–π–ª–∞ –∏–ª–∏ —Ä—É—á–Ω–æ–π
        if chosen != "(manual)" and chosen in self.brands:
            profile = dict(self.brands[chosen])
        else:
            profile = {
                "brand_name": st.session_state.get("manual_brand_name", "BRAND"),
                "description": st.session_state.get("manual_description", ""),
                "aliases": st.session_state.get("manual_aliases", []),
                "sure_drop_patterns": st.session_state.get("manual_sure_drop_patterns", []),
                "pr_reply_markers": st.session_state.get("manual_pr_reply_markers", []),
                "brand_sure_drop": st.session_state.get("manual_brand_sure_drop", []),
                "homonym_noise": st.session_state.get("manual_homonym_noise", []),
            }

        profile = normalize_profile(profile)

        st.subheader("–ö–∞—Ä—Ç–æ—á–∫–∞ –±—Ä–µ–Ω–¥–∞")

        col1, col2 = st.columns([1, 1])
        with col1:
            brand_name = st.text_input(
                "Brand name", value=profile.get("brand_name", "BRAND"))
        with col2:
            aliases_raw = st.text_input(
                "Aliases (—á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é)",
                value=", ".join(profile.get("aliases") or []),
                placeholder="familia, —Ñ–∞–º–∏–ª–∏—è, ...",
            )

        description = st.text_area(
            "–û–ø–∏—Å–∞–Ω–∏–µ –±—Ä–µ–Ω–¥–∞ (–∫–æ–Ω—Ç–µ–∫—Å—Ç)",
            value=profile.get("description", ""),
            height=140,
            placeholder="–ö—Ç–æ —ç—Ç–æ, —á—Ç–æ –ø—Ä–æ–¥–∞—ë—Ç/–¥–µ–ª–∞–µ—Ç, –∫–∞–∫–∏–µ –∫–∞–Ω–∞–ª—ã (–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ/—Å–∞–π—Ç), —á—Ç–æ —Å—á–∏—Ç–∞–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–º‚Ä¶",
        )

        with st.expander("–ë—Ä–µ–Ω–¥-–ø–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ (regex)", expanded=False):
            extra_brand_patterns = st.text_area(
                "–ö–∞–∂–¥–∞—è —Å—Ç—Ä–æ–∫–∞ ‚Äî –æ—Ç–¥–µ–ª—å–Ω—ã–π regex. –î–æ–±–∞–≤–∏—Ç—Å—è –∫ brand_name –∏ aliases.",
                value=st.session_state.get("extra_brand_patterns", ""),
                height=120,
                key="extra_brand_patterns",
                placeholder=r"–Ω–∞–ø—Ä–∏–º–µ—Ä:\n(?<!\w)familia(?!\w)\n(?<!\w)—Ñ–∞–º–∏–ª–∏—è(?!\w)\n",
            )

        with st.expander("–ü—Ä–∞–≤–∏–ª–∞ ¬´—Ç–æ—á–Ω–æ drop¬ª (–±—Ä–µ–Ω–¥-—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ —Ä–µ–≥—ç–∫—Å–ø—ã)"):
            sure_drop_text = st.text_area(
                "–ü–æ –æ–¥–Ω–æ–º—É –ø–∞—Ç—Ç–µ—Ä–Ω—É –≤ —Å—Ç—Ä–æ–∫–µ",
                value="\n".join(profile.get("sure_drop_patterns") or []),
                height=140,
                placeholder=r'(?i)\bsagrada\s+familia\b',
            )

        with st.expander("–ú–∞—Ä–∫–µ—Ä—ã PR/–æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤ (—Ä–µ–≥—ç–∫—Å–ø—ã)"):
            pr_text = st.text_area(
                "–ü–æ –æ–¥–Ω–æ–º—É –ø–∞—Ç—Ç–µ—Ä–Ω—É –≤ —Å—Ç—Ä–æ–∫–µ",
                value="\n".join(profile.get("pr_reply_markers") or []),
                height=120,
                placeholder=r"(?i)^–∑–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ",
            )

        # –æ–±–Ω–æ–≤–∏–º –ø—Ä–æ—Ñ–∏–ª—å
        profile["brand_name"] = brand_name.strip(
        ) if brand_name.strip() else "BRAND"
        profile["aliases"] = [a.strip()
                              for a in aliases_raw.split(",") if a.strip()]
        profile["description"] = description.strip()
        profile["sure_drop_patterns"] = [line.strip()
                                         for line in sure_drop_text.splitlines() if line.strip()]
        profile["pr_reply_markers"] = [line.strip()
                                       for line in pr_text.splitlines() if line.strip()]

        # –¥–ª—è —Å–µ—Ä–≤–∏—Å-–∫–ª–∞—Å—Å–∞: –ø—Ä–æ–∫–∏–Ω–µ–º —Å–æ–≤–º–µ—Å—Ç–∏–º—ã–µ –∫–ª—é—á–∏
        profile["brand_sure_drop"] = profile["sure_drop_patterns"]
        profile.setdefault("homonym_noise", [])

        if chosen == "(manual)":
            st.session_state["manual_brand_name"] = profile["brand_name"]
            st.session_state["manual_aliases"] = profile["aliases"]
            st.session_state["manual_description"] = profile["description"]
            st.session_state["manual_sure_drop_patterns"] = profile["sure_drop_patterns"]
            st.session_state["manual_pr_reply_markers"] = profile["pr_reply_markers"]
            st.session_state["manual_brand_sure_drop"] = profile["brand_sure_drop"]
            st.session_state["manual_homonym_noise"] = profile.get(
                "homonym_noise", [])

        st.markdown("</div>", unsafe_allow_html=True)
        return profile, extra_brand_patterns

    def system_prompt_section(self, profile: Dict[str, Any]) -> str:
        st.markdown('<div class="card">', unsafe_allow_html=True)
        st.subheader("System prompt template (—à–∞–±–ª–æ–Ω)")

        base_template = st.text_area(
            "–®–∞–±–ª–æ–Ω (–º–æ–∂–Ω–æ –ø—Ä–∞–≤–∏—Ç—å)",
            value=st.session_state.get("base_template", BASE_SYSTEM_TEMPLATE),
            height=220,
        )
        st.session_state["base_template"] = base_template

        final_system = format_system_prompt(base_template, profile)
        with st.expander("Preview: –∏—Ç–æ–≥–æ–≤—ã–π system prompt"):
            st.code(final_system, language="text")

        st.markdown("</div>", unsafe_allow_html=True)
        return final_system

    def ensure_ready(self):
        if _IMPORT_ERR is not None:
            st.error("–ù–µ –Ω–∞–π–¥–µ–Ω—ã —Ñ–∞–π–ª—ã llm_model.py –∏/–∏–ª–∏ filter_service.py.")
            st.code(str(_IMPORT_ERR), language="text")
            st.stop()

        if not self.api_key_present:
            st.warning(
                "–ù–µ –Ω–∞–π–¥–µ–Ω OPENAI_API_KEY. –î–æ–±–∞–≤—å –∫–ª—é—á –≤ env –∏–ª–∏ Streamlit secrets.")
            # UI –æ—Å—Ç–∞–≤–ª—è–µ–º, –Ω–æ –∑–∞–ø—É—Å–∫ –±–ª–æ–∫–∏—Ä—É–µ–º –ø–æ –∫–Ω–æ–ø–∫–µ

        if _SENTIMENT_IMPORT_ERR is not None:
            # –Ω–µ —Å—Ç–æ–ø–∞–µ–º ‚Äî –ø—Ä–æ—Å—Ç–æ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ (—Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–∞)
            st.info(
                "Sentiment-–º–æ–¥—É–ª—å –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω (deps/–∏–º–ø–æ—Ä—Ç—ã). –í–∫–ª—é—á–µ–Ω–∏–µ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ –±—É–¥–µ—Ç –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å—Å—è.")

    def _maybe_build_sentiment_service(
        self,
        *,
        preproc_factory: PreprocessorFactory,
        pre: proc.CommentPreprocessor,
        artifacts_npz: str,
        openai_embed_model: str,
        embed_batch_size: int,
    ) -> Optional[SentimentService]:
        if not _sentiment_available():
            return None

        api_key = get_api_key()
        if not api_key:
            return None

        if not Path(artifacts_npz).exists():
            return None

        def preprocess_fn(text_rule: str) -> str:
            return preproc_factory.preprocess_for_llm(text_rule, pre)

        try:
            sent_model = get_sentiment_model_cached(
                api_key=api_key,
                artifacts_npz=artifacts_npz,
                openai_embed_model=openai_embed_model,
                dimensions=None,
            )
            return SentimentService(
                model=sent_model,
                preprocess_fn=preprocess_fn,
                embed_batch_size=int(embed_batch_size),
            )
        except Exception:
            return None

    def render_single(
        self,
        *,
        profile: Dict[str, Any],
        final_system: str,
        extra_brand_patterns: str,
        model: str,
        temperature: float,
        truncate_chars: int,
        enable_sentiment: bool,
        sentiment_only_kept: bool,
        sentiment_artifacts: str,
        sentiment_embed_model: str,
        sentiment_embed_batch: int,
    ):
        st.subheader("–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π (–æ–¥–∏–Ω)")

        single_text = st.text_area(
            "–í—Å—Ç–∞–≤—å —Ç–µ–∫—Å—Ç",
            height=180,
            placeholder="–û–¥–∏–Ω –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π —Å—é–¥–∞‚Ä¶",
            key="single_text",
        )
        run_one = st.button("üöÄ –ó–∞–ø—É—Å—Ç–∏—Ç—å", type="primary",
                            use_container_width=True, key="run_one_btn")
        st.markdown("</div>", unsafe_allow_html=True)

        if not run_one:
            return

        if not self.api_key_present:
            st.error("–ù–µ—Ç OPENAI_API_KEY ‚Äî –¥–æ–±–∞–≤—å –∫–ª—é—á –≤ Secrets.")
            st.stop()

        # preprocessor –ø–æ–¥ —Ç–µ–∫—É—â–∏–π –ø—Ä–æ—Ñ–∏–ª—å/–ø–∞—Ç—Ç–µ—Ä–Ω—ã
        preproc_factory = PreprocessorFactory(max_words=250)
        pre = preproc_factory.make(profile, extra_brand_patterns)

        def preprocess_fn(text_rule: str) -> str:
            return preproc_factory.preprocess_for_llm(text_rule, pre)

        client = get_client()
        llm = OpenAIRelevanceBatchModel(client=client, default_model=model)
        service = RelevanceFilterService(llm=llm)

        with st.spinner("–§–∏–ª—å—Ç—Ä—É—é‚Ä¶"):
            t0 = time.perf_counter()
            res = service.classify_one(
                raw_text=single_text,
                profile=profile,
                system_prompt=final_system,
                preprocess_fn=preprocess_fn,
                truncate_chars=truncate_chars,
                model=model,
                temperature=temperature,
            )
            total_dt = time.perf_counter() - t0

        action = res.get("action", "keep")
        is_drop = action == "drop"
        source = res.get("source", "llm")

        sent_label = None
        sent_source = None
        sent_scores = None
        sent_sim_pred = None
        sent_skipped_reason = None

        # st.markdown('<div class="card">', unsafe_allow_html=True)
        # st.header("–†–µ–∑—É–ª—å—Ç–∞—Ç (Relevance)")

        # if is_drop:
        #     st.markdown('<span class="badge-drop">DROP</span>',
        #                 unsafe_allow_html=True)
        # else:
        #     st.markdown('<span class="badge-keep">KEEP</span>',
        #                 unsafe_allow_html=True)

        # if source == "rule":
        #     st.markdown(' <span class="badge-rule">RULE</span>',
        #                 unsafe_allow_html=True)
        #     st.caption(
        #         f"Pre-LLM –ø—Ä–∞–≤–∏–ª–æ: {res.get('rule', {}).get('rule_code', 'rule')}")
        # else:
        #     st.caption(
        #         f"Latency (batch): {res.get('latency_s', 0.0):.3f}s ‚Ä¢ total: {total_dt:.3f}s")

        # # –ó–∞–∫—Ä—ã–≤–∞–µ–º –∫–∞—Ä—Ç–æ—á–∫—É Relevance
        # st.markdown("</div>", unsafe_allow_html=True)

        # ---------------- Sentiment after relevance ----------------
        if enable_sentiment:
            if sentiment_only_kept and is_drop:
                sent_skipped_reason = "–ø—Ä–æ–ø—É—â–µ–Ω–æ (is_drop=Yes)"
            elif not _sentiment_available():
                sent_skipped_reason = "–º–æ–¥—É–ª—å –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω"
            else:
                sent_service = self._maybe_build_sentiment_service(
                    preproc_factory=preproc_factory,
                    pre=pre,
                    artifacts_npz=sentiment_artifacts,
                    openai_embed_model=sentiment_embed_model,
                    embed_batch_size=sentiment_embed_batch,
                )
                if sent_service is None:
                    sent_skipped_reason = "–Ω–µ —É–¥–∞–ª–æ—Å—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å"
                else:
                    with st.spinner("Sentiment inference‚Ä¶"):
                        sres = sent_service.predict_one(single_text)

                    sent_label = sres.get("label")
                    sent_source = sres.get("source")
                    sent_scores = sres.get("scores")
                    sent_sim_pred = sres.get("sim_pred")

        # # JSON –ø—Ä—è—á–µ–º –≤ debug-–±–ª–æ–∫
        # with st.expander("JSON (debug)", expanded=False):
        #     st.json({"results": [{"global_idx": 0, "action": action}]})
        st.markdown('<div class="card">', unsafe_allow_html=True)
        st.header("–†–µ–∑—É–ª—å—Ç–∞—Ç")

        # --- Relevance badge ---
        # –±–µ–π–¥–∂ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
        badges = []

        # Relevance badge
        if is_drop:
            badges.append(
                '<span class="badge badge--red">RELEVANCE: DROP</span>')
        else:
            badges.append(
                '<span class="badge badge--green">RELEVANCE: KEEP</span>')

        # Sentiment badge
        if enable_sentiment and sent_label:
            if sent_label == "positive":
                badges.append(
                    '<span class="badge badge--green">SENTIMENT: POSITIVE</span>')
            elif sent_label == "negative":
                badges.append(
                    '<span class="badge badge--red">SENTIMENT: NEGATIVE</span>')
            else:
                badges.append(
                    '<span class="badge badge--gray">SENTIMENT: NEUTRAL</span>')

        st.markdown(
            f'<div class="badge-row">{"".join(badges)}</div>', unsafe_allow_html=True)

        # meta
        if source == "rule":
            st.caption(
                f"Pre-LLM –ø—Ä–∞–≤–∏–ª–æ: {res.get('rule', {}).get('rule_code', 'rule')}")
        else:
            st.caption(
                f"Latency (batch): {res.get('latency_s', 0.0):.3f}s ‚Ä¢ total: {total_dt:.3f}s")

        # sentiment meta
        if enable_sentiment and sent_label:
            st.caption(
                f"Sentiment source: {sent_source} ‚Ä¢ sim_pred: {float(sent_sim_pred):.3f}")

        # debug JSON
        with st.expander("JSON (debug)", expanded=False):
            st.json({"results": [{"global_idx": 0, "action": action}]})
            if enable_sentiment and sent_scores is not None:
                st.json({"sentiment_scores": sent_scores})

        st.markdown("</div>", unsafe_allow_html=True)

    def render_file(
        self,
        *,
        profile: Dict[str, Any],
        final_system: str,
        extra_brand_patterns: str,
        model: str,
        temperature: float,
        truncate_chars: int,
        batch_size: int,
        max_workers: int,
        enable_sentiment: bool,
        sentiment_only_kept: bool,
        sentiment_artifacts: str,
        sentiment_embed_model: str,
        sentiment_embed_batch: int,
    ):
        st.subheader("–ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–∞")
        uploaded = st.file_uploader(
            "CSV –∏–ª–∏ Excel. –û–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–π —Å—Ç–æ–ª–±–µ—Ü: –¢–µ–∫—Å—Ç", type=["csv", "xlsx", "xls"])
        run_file = st.button("üöÄ –û–±—Ä–∞–±–æ—Ç–∞—Ç—å —Ñ–∞–π–ª",
                             type="primary", use_container_width=True)

        if not run_file:
            return

        if not self.api_key_present:
            st.error("–ù–µ—Ç OPENAI_API_KEY ‚Äî –¥–æ–±–∞–≤—å –∫–ª—é—á –≤ Secrets.")
            st.stop()

        if uploaded is None:
            st.error("–ó–∞–≥—Ä—É–∑–∏ —Ñ–∞–π–ª CSV/XLSX.")
            st.stop()

        try:
            df_in = read_uploaded_table(uploaded)
        except Exception as e:
            st.error("–ù–µ —Å–º–æ–≥ –ø—Ä–æ—á–∏—Ç–∞—Ç—å —Ñ–∞–π–ª. –ü—Ä–æ–≤–µ—Ä—å —Ñ–æ—Ä–º–∞—Ç CSV/XLSX.")
            st.exception(e)
            st.stop()

        if "–¢–µ–∫—Å—Ç" not in df_in.columns:
            st.error(
                f"–í —Ñ–∞–π–ª–µ –Ω–µ—Ç —Å—Ç–æ–ª–±—Ü–∞ '–¢–µ–∫—Å—Ç'. –ï—Å—Ç—å: {list(df_in.columns)}")
            st.stop()

        texts = df_in["–¢–µ–∫—Å—Ç"].astype(str).tolist()

        preproc_factory = PreprocessorFactory(max_words=250)
        pre = preproc_factory.make(profile, extra_brand_patterns)

        def preprocess_fn(text_rule: str) -> str:
            return preproc_factory.preprocess_for_llm(text_rule, pre)

        client = get_client()
        llm = OpenAIRelevanceBatchModel(client=client, default_model=model)
        service = RelevanceFilterService(llm=llm)

        with st.spinner("–§–∏–ª—å—Ç—Ä—É—é (RULE + LLM –±–∞—Ç—á–∞–º–∏)‚Ä¶"):
            actions, stats = service.classify_many_parallel(
                texts=texts,
                profile=profile,
                system_prompt=final_system,
                preprocess_fn=preprocess_fn,
                batch_size=batch_size,
                max_workers=max_workers,
                truncate_chars=truncate_chars,
                model=model,
                temperature=temperature,
            )

        is_drop = ["Yes" if a == "drop" else "No" for a in actions]
        df_out = pd.DataFrame({"–¢–µ–∫—Å—Ç": texts, "is_drop": is_drop})

        # ---------------- Sentiment after relevance (file) ----------------
        if enable_sentiment:
            if not _sentiment_available():
                st.warning(
                    "Sentiment: –º–æ–¥—É–ª—å –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω (–∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏/–∏–º–ø–æ—Ä—Ç).")
            else:
                sent_service = self._maybe_build_sentiment_service(
                    preproc_factory=preproc_factory,
                    pre=pre,
                    artifacts_npz=sentiment_artifacts,
                    openai_embed_model=sentiment_embed_model,
                    embed_batch_size=sentiment_embed_batch,
                )
                if sent_service is None:
                    st.warning(
                        "Sentiment: –Ω–µ —É–¥–∞–ª–æ—Å—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å (–Ω–µ—Ç –∫–ª—é—á–∞/–∞—Ä—Ç–µ—Ñ–∞–∫—Ç–∞ –∏–ª–∏ –æ—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏).")
                else:
                    idxs, to_score = [], []
                    for i, t in enumerate(texts):
                        if sentiment_only_kept and df_out.loc[i, "is_drop"] == "Yes":
                            continue
                        idxs.append(i)
                        to_score.append(t)

                    sentiment_col = [""] * len(texts)
                    sentiment_source_col = [""] * len(texts)

                    if idxs:
                        with st.spinner("–°—á–∏—Ç–∞—é —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å‚Ä¶"):
                            labels, sources = sent_service.predict_many(
                                to_score)

                        for i, lab, src in zip(idxs, labels, sources):
                            sentiment_col[i] = str(lab)
                            sentiment_source_col[i] = str(src)

                    df_out["sentiment"] = sentiment_col
                    df_out["sentiment_source"] = sentiment_source_col

        st.markdown('<div class="card">', unsafe_allow_html=True)
        st.subheader("–ì–æ—Ç–æ–≤–æ")

        base_caption = (
            f"–°—Ç—Ä–æ–∫: {stats.get('n', len(texts))} ‚Ä¢ RULE drops: {stats.get('rule_drops', 0)} ‚Ä¢ "
            f"LLM calls: {stats.get('llm_calls', 0)} ‚Ä¢ Total: {stats.get('total_s', 0.0):.2f}s ‚Ä¢ "
            f"comments/s: {stats.get('comments_per_s', 0.0) or 0.0:.2f}"
        )
        st.caption(base_caption)

        st.dataframe(df_out.head(20), use_container_width=True)

        name = (uploaded.name or "result").lower()
        out_fmt = "xlsx" if name.endswith((".xlsx", ".xls")) else "csv"
        out_bytes, mime = df_to_download_bytes(df_out, out_fmt=out_fmt)
        out_name = f"filtered_{Path(uploaded.name).stem}.{out_fmt}"

        st.download_button(
            "‚¨áÔ∏è –°–∫–∞—á–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç",
            data=out_bytes,
            file_name=out_name,
            mime=mime,
            use_container_width=True,
        )
        st.markdown("</div>", unsafe_allow_html=True)

    def run(self):
        st.title("Brand Analytics (MVP)")
        st.caption(
            "–°–µ–π—á–∞—Å: Relevance (RULE + LLM) + Sentiment (–ø—Ä–æ—Ç–æ—Ç–∏–ø—ã) –ø–æ—Å–ª–µ relevance. "
            "–î–∞–ª—å—à–µ –¥–æ–±–∞–≤–∏–º —Å–º—ã—Å–ª–æ–≤—ã–µ —Ç–µ–≥–∏."
        )

        self.ensure_ready()

        settings = self.sidebar_settings()
        profile, extra_brand_patterns = self.brand_profile_editor(
            settings["chosen"])
        final_system = self.system_prompt_section(profile)

        mode = st.radio("–†–µ–∂–∏–º", ["–û–¥–∏–Ω –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π",
                        "–§–∞–π–ª (CSV/XLSX)"], horizontal=True)

        if mode == "–§–∞–π–ª (CSV/XLSX)":
            self.render_file(
                profile=profile,
                final_system=final_system,
                extra_brand_patterns=extra_brand_patterns,
                model=settings["model"],
                temperature=settings["temperature"],
                truncate_chars=settings["truncate_chars"],
                batch_size=settings["batch_size"],
                max_workers=settings["max_workers"],
                enable_sentiment=settings["enable_sentiment"],
                sentiment_only_kept=settings["sentiment_only_kept"],
                sentiment_artifacts=settings["sentiment_artifacts"],
                sentiment_embed_model=settings["sentiment_embed_model"],
                sentiment_embed_batch=settings["sentiment_embed_batch"],
            )
        else:
            self.render_single(
                profile=profile,
                final_system=final_system,
                extra_brand_patterns=extra_brand_patterns,
                model=settings["model"],
                temperature=settings["temperature"],
                truncate_chars=settings["truncate_chars"],
                enable_sentiment=settings["enable_sentiment"],
                sentiment_only_kept=settings["sentiment_only_kept"],
                sentiment_artifacts=settings["sentiment_artifacts"],
                sentiment_embed_model=settings["sentiment_embed_model"],
                sentiment_embed_batch=settings["sentiment_embed_batch"],
            )


StreamlitRelevanceApp().run()
